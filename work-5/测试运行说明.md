# 测试运行结果及说明

## 运行结果

<img src=".\img\image-20211221174444469.png" alt="image-20211221174444469" style="zoom:50%;" />

<img src=".\img\image-20211221174527272.png" alt="image-20211221174527272" style="zoom:50%;" />

<img src=".\img\image-20211221174640503.png" alt="image-20211221174640503" style="zoom:50%;" />



实现bp三层神经网络，采用SGD随机梯度下降法，并用5倍交叉验证得到的结果如上图所示

### 实现5倍交叉验证

首先将x和y合并，打乱数据后，利用Sklearn的KFold实现分组

<img src=".\img\image-20211220224720950.png" alt="image-20211220224720950" style="zoom:67%;" />

然后利用随机梯度下降法实现 手写的三层神经网络, BP算法实现的代码在bpnn.py中已经写出，由于长度限制不再此截屏展示。

![image-20211220224943096](.\img\image-20211220224943096.png)

最后利用二分类的评价指标：精确率Precision、召回率Recall、F值F1；分别求出五倍交叉验证后的平均值

![image-20211220225318523](.\img\image-20211220225318523.png)

### 与Logistic和Softmax比较

（1）logistic回归的本质：

　　逻辑回归的激活函数是sigmoid函数，可以理解为一个被sigmoid函数归一化后的线性回归；

（2）softmax和逻辑回归的关系：

　　softmax 回归是逻辑回归的一般形式。当类别数k = 2 时，softmax 回归退化为逻辑回归；logistic处理二分类问题，softmax处理多分类问题；softmax是logistic在多分类问题上的推广；

（3）logistic,softmax与神经网络的关系：

　　logistic回归可看作只有输入层和输出层的神经网络；

　　在神经网络中的最后一层隐含层和输出层就可以看成是logistic回归或者softmax回归模型，之前的隐藏层只是从原始输入数据中学习特征，然后把学习得到的特征交给logistic回归或者softmax回归处理。

　　softmax在神经网络中的作用相当于把输出转换成我们想要的格式，也就是每一个取值对应的0～1之间的概率。

　　因此，可以把处理分类问题的神经网络分成两部分，特征学习和logistic回归或softmax回归。