# Logistic

## 批量梯度下降（GD）

<img src=".\img\image-20211201163918889.png" alt="image-20211201163918889" style="zoom:50%;" />

## 随机梯度下降（SGD）

<img src=".\img\image-20211201164005963.png" alt="image-20211201164005963" style="zoom:50%;" />

## 训练集结果

为了比较批量梯度下降和随机梯度下降，最后的收敛规则设置为当前损失函数值比上次损失函数值小于1e-6时收敛，

结果如图：可见**随机梯度下降（SGD）的速度比批量梯度下降（GD）的速度快很多**

<img src=".\img\image-20211201164229967.png" alt="image-20211201164229967" style="zoom: 50%;" />

<img src=".\img\image-20211201164303620.png" alt="image-20211201164303620" style="zoom:50%;" />

- 蓝线：采用批量梯度下降
- 橙线：采用随机梯度下降
- 可见二者结果都比较好

<img src=".\img\image-20211201163638417.png" alt="image-20211201163638417" style="zoom: 50%;" />

# softmax

## 批量梯度下降（GD）

<img src=".\img\image-20211201164720752.png" alt="image-20211201164720752" style="zoom:50%;" />

## 随机梯度下降（SGD）

<img src=".\img\image-20211201164825337.png" alt="image-20211201164825337" style="zoom:50%;" />

## 训练集结果

<img src=".\img\image-20211201164945505.png" alt="image-20211201164945505" style="zoom:33%;" />

<img src=".\img\image-20211201165025393.png" alt="image-20211201165025393" style="zoom:50%;" />

# 总结

​	对于二分类问题来说，采用logistic回归和softmax回归都能得到比较好的结果，softmax解决多分类问题，logistic解决二分类问题，理论上如果对于求解二分类的问题时，二者没太大区别。